{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOS4fFMJPmnLWxEUQr5vFiZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RohithThiru/AI-projects/blob/main/Quality-of%20GAN-Generated-Images-Using-FID.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEGHWJvkejmB"
      },
      "outputs": [],
      "source": [
        "# Cell 1: installs & imports\n",
        "!pip install -q tensorflow==2.12.0 tensorflow-hub==0.14.0 pillow matplotlib tqdm\n",
        "\n",
        "import os, io, zipfile, math, random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input as inception_preprocess\n",
        "from tensorflow.keras.preprocessing import image as kimage\n",
        "from google.colab import files\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: utilities\n",
        "def load_image_paths_from_dir(dir_path, exts=(\"png\",\"jpg\",\"jpeg\")):\n",
        "    files = []\n",
        "    for root, _, fnames in os.walk(dir_path):\n",
        "        for f in fnames:\n",
        "            if f.lower().split('.')[-1] in exts:\n",
        "                files.append(os.path.join(root, f))\n",
        "    files.sort()\n",
        "    return files\n",
        "\n",
        "def load_and_preprocess(img_path, target_size=(299,299)):\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    img = img.resize(target_size, Image.BICUBIC)\n",
        "    arr = np.asarray(img).astype(\"float32\")\n",
        "    return arr\n",
        "\n",
        "def batch_load_images(paths, target_size=(299,299), max_images=None):\n",
        "    if max_images:\n",
        "        paths = paths[:max_images]\n",
        "    imgs = [load_and_preprocess(p, target_size) for p in paths]\n",
        "    return np.stack(imgs, axis=0)\n",
        "\n",
        "def show_grid(real_imgs, gen_imgs, n=8, title=\"\"):\n",
        "    # expects arrays in [0,255] or [0,1] floats; convert to [0,1]\n",
        "    def norm(x):\n",
        "        x = np.clip(x, 0, 255)\n",
        "        if x.max() > 1.0:\n",
        "            x = x / 255.0\n",
        "        return x\n",
        "    real_imgs = norm(real_imgs)\n",
        "    gen_imgs = norm(gen_imgs)\n",
        "    plt.figure(figsize=(16, 4))\n",
        "    for i in range(n):\n",
        "        plt.subplot(2, n, i+1)\n",
        "        plt.imshow(real_imgs[i])\n",
        "        plt.axis(\"off\")\n",
        "        if i==0: plt.title(\"Real\")\n",
        "        plt.subplot(2, n, n+i+1)\n",
        "        plt.imshow(gen_imgs[i])\n",
        "        plt.axis(\"off\")\n",
        "        if i==0: plt.title(\"Generated\")\n",
        "    plt.suptitle(title)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Po0HtkahekMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: FID computation utilities\n",
        "from scipy import linalg\n",
        "\n",
        "# load InceptionV3 up to pool_3 (global average pooling)\n",
        "def get_inception_model():\n",
        "    base = InceptionV3(include_top=False, weights='imagenet', pooling='avg', input_shape=(299,299,3))\n",
        "    return base\n",
        "\n",
        "inception_model = get_inception_model()\n",
        "\n",
        "def get_activations(images, batch_size=32):\n",
        "    # images: numpy array with shape (N, H, W, 3), values in [0,255] or [0,1]\n",
        "    x = images.copy().astype(\"float32\")\n",
        "    if x.max() <= 1.0:\n",
        "        x = x*255.0\n",
        "    # preprocess as Inception expects\n",
        "    x = inception_preprocess(x)\n",
        "    acts = inception_model.predict(x, batch_size=batch_size, verbose=0)\n",
        "    return acts\n",
        "\n",
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    # from original FID implementation\n",
        "    mu1 = np.atleast_1d(mu1)\n",
        "    mu2 = np.atleast_1d(mu2)\n",
        "    sigma1 = np.atleast_2d(sigma1)\n",
        "    sigma2 = np.atleast_2d(sigma2)\n",
        "    diff = mu1 - mu2\n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if not np.isfinite(covmean).all():\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "    tr_covmean = np.trace(covmean)\n",
        "    fid = diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2*tr_covmean\n",
        "    return float(fid)\n",
        "\n",
        "def compute_fid_from_images(real_images, gen_images, batch_size=32):\n",
        "    acts1 = get_activations(real_images, batch_size=batch_size)\n",
        "    acts2 = get_activations(gen_images, batch_size=batch_size)\n",
        "    mu1, sigma1 = acts1.mean(axis=0), np.cov(acts1, rowvar=False)\n",
        "    mu2, sigma2 = acts2.mean(axis=0), np.cov(acts2, rowvar=False)\n",
        "    fid = calculate_frechet_distance(mu1, sigma1, mu2, sigma2)\n",
        "    return fid\n"
      ],
      "metadata": {
        "id": "X7cWia-SemzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: MODE selection\n",
        "# MODE = \"UPLOAD\"  -> you upload a zip/folder of generated images (recommended)\n",
        "# MODE = \"DEMO\"    -> notebook trains a tiny DCGAN quickly to produce samples (for demo only)\n",
        "MODE = \"UPLOAD\"  # change to \"DEMO\" to run demo generator training\n",
        "\n",
        "# Choose which real dataset to compare against (CIFAR10 or CelebA if available)\n",
        "REAL_DATASET = \"CIFAR10\"  # \"CIFAR10\"\n",
        "\n",
        "# helper to fetch real images (CIFAR10)\n",
        "if REAL_DATASET == \"CIFAR10\":\n",
        "    (xtr, _), (xte, _) = tf.keras.datasets.cifar10.load_data()\n",
        "    # use a reasonable subset as real reference (e.g., 5000 images)\n",
        "    real_images_full = xtr.astype(\"float32\")\n",
        "    print(\"CIFAR10 loaded, total real images available:\", real_images_full.shape[0])\n",
        "else:\n",
        "    raise ValueError(\"Unsupported dataset\")\n"
      ],
      "metadata": {
        "id": "neR1QeBSepw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5A: Upload generated images (zipped) - run if MODE==\"UPLOAD\"\n",
        "if MODE != \"UPLOAD\":\n",
        "    print(\"Skipping upload cell (MODE != UPLOAD)\")\n",
        "else:\n",
        "    print(\"Please upload a zip file of generated images (images inside root of zip).\")\n",
        "    uploaded = files.upload()  # uses browser file chooser\n",
        "    # handle uploaded file\n",
        "    zip_path = None\n",
        "    for fn in uploaded:\n",
        "        if fn.lower().endswith(\".zip\"):\n",
        "            zip_path = fn\n",
        "            break\n",
        "    if zip_path is None:\n",
        "        raise RuntimeError(\"Please upload a zip file of generated images (.zip).\")\n",
        "    extract_dir = \"/content/gen_images\"\n",
        "    os.makedirs(extract_dir, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        z.extractall(extract_dir)\n",
        "    print(\"Extracted to\", extract_dir)\n",
        "    gen_paths = load_image_paths_from_dir(extract_dir)\n",
        "    print(\"Found generated images:\", len(gen_paths))\n",
        "    # load a subset (or all if small)\n",
        "    N_GEN = min(len(gen_paths), 5000)\n",
        "    gen_images = batch_load_images(gen_paths, target_size=(299,299), max_images=N_GEN)\n",
        "    # sample same number of real images randomly\n",
        "    real_sample_indices = np.random.choice(real_images_full.shape[0], N_GEN, replace=False)\n",
        "    real_subset = real_images_full[real_sample_indices]\n",
        "    # resize real images to 299x299\n",
        "    real_resized = np.stack([np.asarray(Image.fromarray(img.astype('uint8')).resize((299,299), Image.BICUBIC)) for img in real_subset], axis=0)\n",
        "    print(\"Prepared real and generated arrays:\", real_resized.shape, gen_images.shape)\n"
      ],
      "metadata": {
        "id": "NeRL_iJuer6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5B: Quick demo GAN training to generate sample images (MODE == \"DEMO\")\n",
        "if MODE != \"DEMO\":\n",
        "    print(\"Skipping demo GAN training (MODE != DEMO)\")\n",
        "else:\n",
        "    # Simple DCGAN on CIFAR10 -- very small and fast for demonstration only\n",
        "    import tensorflow.keras as keras\n",
        "    IMG_SHAPE = (32,32,3)\n",
        "    LATENT_DIM = 100\n",
        "\n",
        "    # prepare CIFAR images scaled [-1,1]\n",
        "    x_real = (real_images_full / 127.5) - 1.0\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(x_real).shuffle(10000).batch(128)\n",
        "\n",
        "    # generator\n",
        "    def build_generator():\n",
        "        model = keras.Sequential([\n",
        "            keras.layers.Dense(8*8*128, input_dim=LATENT_DIM),\n",
        "            keras.layers.Reshape((8,8,128)),\n",
        "            keras.layers.BatchNormalization(),\n",
        "            keras.layers.UpSampling2D(),\n",
        "            keras.layers.Conv2D(128, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "            keras.layers.BatchNormalization(),\n",
        "            keras.layers.UpSampling2D(),\n",
        "            keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "            keras.layers.Conv2D(3, kernel_size=3, padding=\"same\", activation=\"tanh\")\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    # discriminator\n",
        "    def build_discriminator():\n",
        "        model = keras.Sequential([\n",
        "            keras.layers.Conv2D(64, 3, strides=2, input_shape=IMG_SHAPE, padding=\"same\"),\n",
        "            keras.layers.LeakyReLU(0.2),\n",
        "            keras.layers.Dropout(0.3),\n",
        "            keras.layers.Conv2D(128, 3, strides=2, padding=\"same\"),\n",
        "            keras.layers.LeakyReLU(0.2),\n",
        "            keras.layers.Dropout(0.3),\n",
        "            keras.layers.Flatten(),\n",
        "            keras.layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    gen = build_generator()\n",
        "    disc = build_discriminator()\n",
        "    bce = keras.losses.BinaryCrossentropy()\n",
        "    gen_opt = keras.optimizers.Adam(0.0002, 0.5)\n",
        "    disc_opt = keras.optimizers.Adam(0.0002, 0.5)\n",
        "\n",
        "    # training loop (very short)\n",
        "    EPOCHS = 6\n",
        "    for epoch in range(EPOCHS):\n",
        "        for real_batch in dataset.take(300):  # limit batches so it's quick\n",
        "            bs = real_batch.shape[0]\n",
        "            noise = tf.random.normal((bs, LATENT_DIM))\n",
        "            fake = gen(noise, training=True)\n",
        "            # train disc\n",
        "            with tf.GradientTape() as tape:\n",
        "                real_logits = disc(real_batch, training=True)\n",
        "                fake_logits = disc(fake, training=True)\n",
        "                d_loss = bce(tf.ones_like(real_logits), real_logits) + bce(tf.zeros_like(fake_logits), fake_logits)\n",
        "            grads = tape.gradient(d_loss, disc.trainable_variables)\n",
        "            disc_opt.apply_gradients(zip(grads, disc.trainable_variables))\n",
        "            # train gen\n",
        "            noise = tf.random.normal((bs, LATENT_DIM))\n",
        "            with tf.GradientTape() as tape2:\n",
        "                gen_imgs = gen(noise, training=True)\n",
        "                logits = disc(gen_imgs, training=True)\n",
        "                g_loss = bce(tf.ones_like(logits), logits)\n",
        "            grads2 = tape2.gradient(g_loss, gen.trainable_variables)\n",
        "            gen_opt.apply_gradients(zip(grads2, gen.trainable_variables))\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS} done\")\n",
        "\n",
        "    # generate samples\n",
        "    N_GEN = 1000\n",
        "    noise = tf.random.normal((N_GEN, LATENT_DIM))\n",
        "    gen_out = gen.predict(noise, batch_size=64)\n",
        "    # gen_out in [-1,1] -> convert to [0,255] uint8\n",
        "    gen_out = ((gen_out + 1.0) * 127.5).astype('uint8')\n",
        "    # resize to 299x299 for Inception\n",
        "    gen_images = np.stack([np.asarray(Image.fromarray(img).resize((299,299), Image.BICUBIC)) for img in gen_out], axis=0)\n",
        "    # sample same number of real images\n",
        "    real_subset_idx = np.random.choice(real_images_full.shape[0], N_GEN, replace=False)\n",
        "    real_subset = real_images_full[real_subset_idx]\n",
        "    real_resized = np.stack([np.asarray(Image.fromarray(img.astype('uint8')).resize((299,299), Image.BICUBIC)) for img in real_subset], axis=0)\n",
        "    print(\"Generated demo images:\", gen_images.shape)\n"
      ],
      "metadata": {
        "id": "n3PuO0rRetoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "assert 'gen_images' in globals() and 'real_resized' in globals(), \"gen_images or real_resized not prepared. Run upload or demo steps.\"\n",
        "\n",
        "gen_images_arr = gen_images.astype(\"float32\")\n",
        "real_images_arr = real_resized.astype(\"float32\")\n",
        "N = min(real_images_arr.shape[0], gen_images_arr.shape[0], 2000)\n",
        "print(\"Using N images per set for FID:\", N)\n",
        "real_subset_small = real_images_arr[:N]\n",
        "gen_subset_small = gen_images_arr[:N]\n",
        "\n",
        "show_grid(real_subset_small/255.0, gen_subset_small/255.0, n=8, title=\"Real vs Generated samples (first 8)\")\n",
        "\n",
        "\n",
        "print(\"Computing FID (this may take some time)...\")\n",
        "fid_value = compute_fid_from_images(real_subset_small, gen_subset_small, batch_size=32)\n",
        "print(\"FID:\", fid_value)\n"
      ],
      "metadata": {
        "id": "Q2fk3zMEevp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"fid_report.pdf\"\n",
        "with PdfPages(pdf_path) as pdf:\n",
        "    fig = plt.figure(figsize=(8.27, 11.69))\n",
        "    plt.axis(\"off\")\n",
        "    plt.text(0.01, 0.92, \"FID Evaluation Report\", fontsize=18, weight='bold')\n",
        "    plt.text(0.01, 0.86, f\"Mode: {MODE}    Real dataset: {REAL_DATASET}\", fontsize=10)\n",
        "    plt.text(0.01, 0.80, \"Metric: Fr√©chet Inception Distance (FID)\\n\\nFID measures the distance between multivariate Gaussians fit to Inception features of real and generated images. Lower is better (closer to real data distribution).\", fontsize=10)\n",
        "    plt.text(0.01, 0.55, f\"Computed FID (N={N}): {fid_value:.4f}\", fontsize=12)\n",
        "    plt.text(0.01, 0.45, \"Notes:\\n- Use at least ~1k-5k images for stable FID.\\n- This report contains sample visuals and the computed FID.\", fontsize=10)\n",
        "    pdf.savefig(fig, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "    fig = plt.figure(figsize=(11,5))\n",
        "    M = 8\n",
        "    for i in range(M):\n",
        "        plt.subplot(2, M, i+1)\n",
        "        plt.imshow(real_subset_small[i].astype('uint8'))\n",
        "        plt.axis(\"off\")\n",
        "        if i==0: plt.title(\"Real\")\n",
        "        plt.subplot(2, M, M+i+1)\n",
        "        plt.imshow(gen_subset_small[i].astype('uint8'))\n",
        "        plt.axis(\"off\")\n",
        "        if i==0: plt.title(\"Generated\")\n",
        "    plt.suptitle(\"Real vs Generated samples (first 8)\")\n",
        "    plt.tight_layout()\n",
        "    pdf.savefig(fig, bbox_inches='tight', dpi=150)\n",
        "    plt.close(fig)\n",
        "\n",
        "    fig = plt.figure(figsize=(8.27, 11.69))\n",
        "    plt.axis(\"off\")\n",
        "    conclusion = (\n",
        "        f\"Conclusions & interpretation\\n\\n- Computed FID: {fid_value:.4f}\\n\\n\"\n",
        "        \"- Interpretation: lower FID indicates generated images are closer to the real distribution in Inception feature space.\\n\"\n",
        "        \"- Use-case notes: for serious evaluation, compute FID with ~5k images and multiple runs.\\n\"\n",
        "        \"- Visual inspection: check the visuals page for obvious artifacts, mode collapse, or diversity issues.\\n\"\n",
        "    )\n",
        "    plt.text(0.01, 0.95, \"Conclusions\", fontsize=14, weight='bold')\n",
        "    plt.text(0.01, 0.6, conclusion, fontsize=10)\n",
        "    pdf.savefig(fig, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "print(\"Saved report to\", pdf_path)\n"
      ],
      "metadata": {
        "id": "Cx9Rq2obexRN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}